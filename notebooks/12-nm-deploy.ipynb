{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inflection\n",
    "import umap as umap\n",
    "import re\n",
    "import s3fs\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition as dd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/s3fs/core.py:113\u001b[0m, in \u001b[0;36m_error_wrapper\u001b[0;34m(func, args, kwargs, retries)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3_RETRYABLE_ERRORS \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/aiobotocore/client.py:408\u001b[0m, in \u001b[0;36mAioBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    407\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (403) when calling the HeadObject operation: Forbidden",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m path_s3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3://insiders-dataset-nm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel( path_s3 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Online Retail.xlsx/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[1;32m    496\u001b[0m         io,\n\u001b[1;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/pandas/io/excel/_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[1;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m   1552\u001b[0m     )\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/pandas/io/excel/_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m   1403\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    729\u001b[0m     path_or_buf,\n\u001b[1;32m    730\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    731\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    732\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    733\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    734\u001b[0m )\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/pandas/io/common.py:443\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    439\u001b[0m             storage_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(storage_options)\n\u001b[1;32m    440\u001b[0m             storage_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manon\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    441\u001b[0m         file_obj \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    442\u001b[0m             filepath_or_buffer, mode\u001b[38;5;241m=\u001b[39mfsspec_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[0;32m--> 443\u001b[0m         )\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[1;32m    446\u001b[0m         filepath_or_buffer\u001b[38;5;241m=\u001b[39mfile_obj,\n\u001b[1;32m    447\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m         mode\u001b[38;5;241m=\u001b[39mfsspec_mode,\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m storage_options:\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/fsspec/core.py:135\u001b[0m, in \u001b[0;36mOpenFile.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    The OpenFile object should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    instances persisting. You must, therefore, keep a reference to the OpenFile\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    during the life of the file-like it generates.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/fsspec/core.py:103\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    101\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfobjects \u001b[38;5;241m=\u001b[39m [f]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/fsspec/spec.py:1293\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1293\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(\n\u001b[1;32m   1294\u001b[0m         path,\n\u001b[1;32m   1295\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   1296\u001b[0m         block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[1;32m   1297\u001b[0m         autocommit\u001b[38;5;241m=\u001b[39mac,\n\u001b[1;32m   1298\u001b[0m         cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/s3fs/core.py:671\u001b[0m, in \u001b[0;36mS3FileSystem._open\u001b[0;34m(self, path, mode, block_size, acl, version_id, fill_cache, cache_type, autocommit, size, requester_pays, cache_options, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    669\u001b[0m     cache_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_cache_type\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m S3File(\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    673\u001b[0m     path,\n\u001b[1;32m    674\u001b[0m     mode,\n\u001b[1;32m    675\u001b[0m     block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[1;32m    676\u001b[0m     acl\u001b[38;5;241m=\u001b[39macl,\n\u001b[1;32m    677\u001b[0m     version_id\u001b[38;5;241m=\u001b[39mversion_id,\n\u001b[1;32m    678\u001b[0m     fill_cache\u001b[38;5;241m=\u001b[39mfill_cache,\n\u001b[1;32m    679\u001b[0m     s3_additional_kwargs\u001b[38;5;241m=\u001b[39mkw,\n\u001b[1;32m    680\u001b[0m     cache_type\u001b[38;5;241m=\u001b[39mcache_type,\n\u001b[1;32m    681\u001b[0m     autocommit\u001b[38;5;241m=\u001b[39mautocommit,\n\u001b[1;32m    682\u001b[0m     requester_pays\u001b[38;5;241m=\u001b[39mrequester_pays,\n\u001b[1;32m    683\u001b[0m     cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[1;32m    684\u001b[0m     size\u001b[38;5;241m=\u001b[39msize,\n\u001b[1;32m    685\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/s3fs/core.py:2110\u001b[0m, in \u001b[0;36mS3File.__init__\u001b[0;34m(self, s3, path, mode, block_size, acl, version_id, fill_cache, s3_additional_kwargs, autocommit, cache_type, requester_pays, cache_options, size)\u001b[0m\n\u001b[1;32m   2108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetails \u001b[38;5;241m=\u001b[39m s3\u001b[38;5;241m.\u001b[39minfo(path)\n\u001b[1;32m   2109\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetails\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersionId\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2110\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   2111\u001b[0m     s3,\n\u001b[1;32m   2112\u001b[0m     path,\n\u001b[1;32m   2113\u001b[0m     mode,\n\u001b[1;32m   2114\u001b[0m     block_size,\n\u001b[1;32m   2115\u001b[0m     autocommit\u001b[38;5;241m=\u001b[39mautocommit,\n\u001b[1;32m   2116\u001b[0m     cache_type\u001b[38;5;241m=\u001b[39mcache_type,\n\u001b[1;32m   2117\u001b[0m     cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[1;32m   2118\u001b[0m     size\u001b[38;5;241m=\u001b[39msize,\n\u001b[1;32m   2119\u001b[0m )\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs  \u001b[38;5;66;03m# compatibility\u001b[39;00m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;66;03m# when not using autocommit we want to have transactional state to manage\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/fsspec/spec.py:1651\u001b[0m, in \u001b[0;36mAbstractBufferedFile.__init__\u001b[0;34m(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, size, **kwargs)\u001b[0m\n\u001b[1;32m   1649\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1651\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetails[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m caches[cache_type](\n\u001b[1;32m   1653\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_range, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcache_options\n\u001b[1;32m   1654\u001b[0m     )\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/fsspec/spec.py:1664\u001b[0m, in \u001b[0;36mAbstractBufferedFile.details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetails\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1664\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/s3fs/core.py:1302\u001b[0m, in \u001b[0;36mS3FileSystem._info\u001b[0;34m(self, path, bucket, key, refresh, version_id)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key:\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_s3(\n\u001b[1;32m   1303\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_object\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1304\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m   1305\u001b[0m             Bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[1;32m   1306\u001b[0m             Key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m   1307\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mversion_id_kw(version_id),\n\u001b[1;32m   1308\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreq_kw,\n\u001b[1;32m   1309\u001b[0m         )\n\u001b[1;32m   1310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m   1311\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m: out\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLastModified\u001b[39m\u001b[38;5;124m\"\u001b[39m: out\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLastModified\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContentType\u001b[39m\u001b[38;5;124m\"\u001b[39m: out\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContentType\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1319\u001b[0m         }\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/s3fs/core.py:348\u001b[0m, in \u001b[0;36mS3FileSystem._call_s3\u001b[0;34m(self, method, *akwarglist, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCALL: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, akwarglist, kw2)\n\u001b[1;32m    347\u001b[0m additional_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_s3_method_kwargs(method, \u001b[38;5;241m*\u001b[39makwarglist, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _error_wrapper(\n\u001b[1;32m    349\u001b[0m     method, kwargs\u001b[38;5;241m=\u001b[39madditional_kwargs, retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries\n\u001b[1;32m    350\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/clusterenv/lib/python3.12/site-packages/s3fs/core.py:140\u001b[0m, in \u001b[0;36m_error_wrapper\u001b[0;34m(func, args, kwargs, retries)\u001b[0m\n\u001b[1;32m    138\u001b[0m         err \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    139\u001b[0m err \u001b[38;5;241m=\u001b[39m translate_boto_error(err)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[0;31mPermissionError\u001b[0m: Forbidden"
     ]
    }
   ],
   "source": [
    "path_s3 = 's3://insiders-dataset-nm'\n",
    "data = pd.read_excel( path_s3 + '/Online Retail.xlsx/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_old = ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate',\n",
    "       'UnitPrice', 'CustomerID', 'Country']\n",
    "\n",
    "snakecase = lambda x: inflection.underscore( x ) # change to snakecase\n",
    "cols_new = list( map( snakecase, cols_old ))\n",
    "\n",
    "data.columns = cols_new # define the new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Check and fillout Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NA\n",
    "df_missing = data[data['customer_id'].isna()]\n",
    "df_not_missing = data[~data['customer_id'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reference\n",
    "backup = pd.DataFrame(df_missing['invoice_no'].drop_duplicates())\n",
    "backup['customer_id'] = np.arange(19000, 19000+len(backup), 1)\n",
    "\n",
    "# merge original data frame\n",
    "df1 = pd.merge(data, backup, on='invoice_no', how='left')\n",
    "\n",
    "#coalesce\n",
    "df1['customer_id'] = df1['customer_id_x'].combine_first(df1['customer_id_y'])\n",
    "\n",
    "# drop columns\n",
    "df1 = df1.drop(columns=['customer_id_x', 'customer_id_y'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Change dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoice data to datetime\n",
    "df1['invoice_date'] = pd.to_datetime(df1['invoice_date'], format='%d-%b-%y')\n",
    "#df1['invoice_date'] = df1['invoice_date'].dt.date\n",
    "\n",
    "# customer id\n",
    "df1['customer_id'] = df1['customer_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Filtragem de variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit price > 0.04\n",
    "df2 = df2.loc[df2['unit_price'] >= 0.04, :]\n",
    "\n",
    "# stock code \n",
    "df2 = df2[~df2['stock_code'].isin(['POST', 'D', 'DOT', 'M', 'S', 'AMAZONFEE', 'm', 'DCGSSBOY', 'DCGSSGIRL','PADS', 'B', 'CRUK'])]\n",
    "\n",
    "# description\n",
    "df2 = df2.drop(columns='description', axis=1)\n",
    "\n",
    "#map\n",
    "df2 = df2[~df2['country'].isin(['European Community', 'Unspecified'])]\n",
    "\n",
    "# bad users\n",
    "df2 = df2[~df2['customer_id'].isin([16446])]\n",
    "\n",
    "# quantity - negative numbers means product returns\n",
    "df2_returns = df2.loc[df2['quantity'] < 0, :]\n",
    "df2_purchases = df2.loc[df2['quantity'] >= 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date reference \n",
    "df_ref = df2_purchases.drop(['invoice_no', 'stock_code',\n",
    "                   'quantity', 'invoice_date', 'unit_price', \n",
    "                   'country'], axis=1).drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5695, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ref.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69667/206990356.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_purchases.loc[:, 'gross_revenue'] = df2_purchases.loc[:, 'quantity'] * df2_purchases.loc[:, 'unit_price']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "customer_id      0\n",
       "gross_revenue    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gross Revenue ( Faturamento ) quantity * price\n",
    "df2_purchases.loc[:, 'gross_revenue'] = df2_purchases.loc[:, 'quantity'] * df2_purchases.loc[:, 'unit_price']\n",
    "\n",
    "# Monetary\n",
    "df_monetary = df2_purchases.loc[:, ['customer_id', 'gross_revenue']].groupby( 'customer_id' ).sum().reset_index()\n",
    "df_ref = pd.merge( df_ref, df_monetary, on='customer_id', how='left' )\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id      0\n",
       "gross_revenue    0\n",
       "recency_days     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recency - Last day purchase\n",
    "df_recency = df2_purchases.loc[:, ['customer_id', 'invoice_date']].groupby( 'customer_id' ).max().reset_index()\n",
    "df_recency['recency_days'] = ( df2['invoice_date'].max() - df_recency['invoice_date'] ).dt.days\n",
    "df_recency['recency_days'] = df_recency['recency_days'].apply( lambda x: 1/x if x !=0 else 1)\n",
    "df_recency = df_recency[['customer_id', 'recency_days']].copy()\n",
    "df_ref = pd.merge( df_ref, df_recency, on='customer_id', how='left' )\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id      0\n",
       "gross_revenue    0\n",
       "recency_days     0\n",
       "qtde_invoices    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qtde_invoices\n",
    "df_freq = (df2_purchases.loc[:, ['customer_id', 'invoice_no']].drop_duplicates()\n",
    "                                                             .groupby( 'customer_id' )\n",
    "                                                             .count()\n",
    "                                                             .reset_index()\n",
    "                                                             .rename( columns={'invoice_no': 'qtde_invoices'}) )\n",
    "df_ref = pd.merge( df_ref, df_freq, on='customer_id', how='left' )\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id      0\n",
       "gross_revenue    0\n",
       "recency_days     0\n",
       "qtde_invoices    0\n",
       "qtde_items       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qtde_items\n",
    "df_freq = (df2_purchases.loc[:, ['customer_id', 'quantity']].groupby( 'customer_id' ).sum()\n",
    "                                                           .reset_index()\n",
    "                                                           .rename( columns={'quantity': 'qtde_items'} ) )\n",
    "df_ref = pd.merge( df_ref, df_freq, on='customer_id', how='left' )\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id      0\n",
       "gross_revenue    0\n",
       "recency_days     0\n",
       "qtde_invoices    0\n",
       "qtde_items       0\n",
       "qtde_products    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de produtos\n",
    "df_freq = (df2_purchases.loc[:, ['customer_id', 'stock_code']].groupby( 'customer_id' ).count()\n",
    "                                                           .reset_index()\n",
    "                                                           .rename( columns={'stock_code': 'qtde_products'} ) )\n",
    "df_ref = pd.merge( df_ref, df_freq, on='customer_id', how='left' )\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id      0\n",
       "gross_revenue    0\n",
       "recency_days     0\n",
       "qtde_invoices    0\n",
       "qtde_items       0\n",
       "qtde_products    0\n",
       "avg_ticket       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avg Ticket\n",
    "df_avg_ticket = df2_purchases.loc[:, ['customer_id', 'gross_revenue']].groupby( 'customer_id' ).mean().reset_index().rename( columns={'gross_revenue':'avg_ticket'} )\n",
    "df_ref = pd.merge( df_ref, df_avg_ticket, on='customer_id', how='left')\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id         0\n",
       "gross_revenue       0\n",
       "recency_days        0\n",
       "qtde_invoices       0\n",
       "qtde_items          0\n",
       "qtde_products       0\n",
       "avg_ticket          0\n",
       "avg_recency_days    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average recency days\n",
    "df_aux = df2[['customer_id', 'invoice_date']].drop_duplicates().sort_values( ['customer_id', 'invoice_date'], ascending= False )\n",
    "df_aux['next_customer_id'] = df_aux['customer_id'].shift() # next customer\n",
    "df_aux['previous_date'] = df_aux['invoice_date'].shift() # next invoince date\n",
    "\n",
    "df_aux['avg_recency_days'] = df_aux.apply( lambda x: ( x['previous_date'] - x['invoice_date'] ).days if x['customer_id'] == x['next_customer_id'] else 0, axis=1 )\n",
    "df_aux['avg_recency_days'] = -1* df_aux['avg_recency_days'].apply( lambda x: 1/x if x!=0 else 1)\n",
    "\n",
    "df_aux = df_aux.drop( ['invoice_date', 'next_customer_id', 'previous_date'], axis=1 ).dropna()\n",
    "\n",
    "# average recency \n",
    "df_avg_recency_days = df_aux.groupby( 'customer_id' ).mean().reset_index()\n",
    "\n",
    "# merge\n",
    "df_ref = pd.merge( df_ref, df_avg_recency_days, on='customer_id', how='left' )\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id         0\n",
       "gross_revenue       0\n",
       "recency_days        0\n",
       "qtde_invoices       0\n",
       "qtde_items          0\n",
       "qtde_products       0\n",
       "avg_ticket          0\n",
       "avg_recency_days    0\n",
       "frequency           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frquency\n",
    "df_aux = ( df2_purchases[['customer_id', 'invoice_no', 'invoice_date']].drop_duplicates()\n",
    "                                                             .groupby( 'customer_id')\n",
    "                                                             .agg( max_ = ( 'invoice_date', 'max' ), \n",
    "                                                                   min_ = ( 'invoice_date', 'min' ),\n",
    "                                                                   days_= ( 'invoice_date', lambda x: ( ( x.max() - x.min() ).days ) + 1 ),\n",
    "                                                                   buy_ = ( 'invoice_no', 'count' ) ) ).reset_index()\n",
    "# Frequency\n",
    "df_aux['frequency'] = df_aux[['buy_', 'days_']].apply( lambda x: x['buy_'] / x['days_'] if  x['days_'] != 0 else 0, axis=1 )\n",
    "\n",
    "# Merge\n",
    "df_ref = pd.merge( df_ref, df_aux[['customer_id', 'frequency']], on='customer_id', how='left' )\n",
    "\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id         0\n",
       "gross_revenue       0\n",
       "recency_days        0\n",
       "qtde_invoices       0\n",
       "qtde_items          0\n",
       "qtde_products       0\n",
       "avg_ticket          0\n",
       "avg_recency_days    0\n",
       "frequency           0\n",
       "qtde_returns        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Returns\n",
    "df_returns = df2_returns[['customer_id', 'quantity']].groupby( 'customer_id' ).sum().reset_index().rename( columns={'quantity':'qtde_returns'} )\n",
    "df_returns['qtde_returns'] = df_returns['qtde_returns'] * -1\n",
    "df_returns['qtde_returns'] = df_returns['qtde_returns'].apply( lambda x: 1/x if x!= 0 else 0)\n",
    "\n",
    "\n",
    "df_ref = pd.merge( df_ref, df_returns, how='left', on='customer_id' )\n",
    "df_ref.loc[df_ref['qtde_returns'].isna(), 'qtde_returns'] = 0\n",
    "\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id         0\n",
       "gross_revenue       0\n",
       "recency_days        0\n",
       "qtde_invoices       0\n",
       "qtde_items          0\n",
       "qtde_products       0\n",
       "avg_ticket          0\n",
       "avg_recency_days    0\n",
       "frequency           0\n",
       "qtde_returns        0\n",
       "avg_basket_size     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg_basket_size\n",
    "df_aux = ( df2_purchases.loc[:, ['customer_id', 'invoice_no', 'quantity']].groupby( 'customer_id' )\n",
    "                                                                            .agg( n_purchase=( 'invoice_no', 'nunique'),\n",
    "                                                                                  n_products=( 'quantity', 'sum' ) )\n",
    "                                                                            .reset_index() )\n",
    "\n",
    "# calculation\n",
    "df_aux['avg_basket_size'] = df_aux['n_products'] / df_aux['n_purchase']\n",
    "\n",
    "# merge\n",
    "df_ref = pd.merge( df_ref, df_aux[['customer_id', 'avg_basket_size']], how='left', on='customer_id' )\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id               0\n",
       "gross_revenue             0\n",
       "recency_days              0\n",
       "qtde_invoices             0\n",
       "qtde_items                0\n",
       "qtde_products             0\n",
       "avg_ticket                0\n",
       "avg_recency_days          0\n",
       "frequency                 0\n",
       "qtde_returns              0\n",
       "avg_basket_size           0\n",
       "avg_unique_basket_size    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg_unique_basket_size\n",
    "df_aux = ( df2_purchases.loc[:, ['customer_id', 'invoice_no', 'stock_code']].groupby( 'customer_id' )\n",
    "                                                                            .agg( n_purchase=( 'invoice_no', 'nunique'),\n",
    "                                                                                   n_products=( 'stock_code', 'nunique' ) )\n",
    "                                                                            .reset_index() )\n",
    "\n",
    "# calculation\n",
    "df_aux['avg_unique_basket_size'] = df_aux['n_products'] / df_aux['n_purchase']\n",
    "\n",
    "# merge\n",
    "df_ref = pd.merge( df_ref, df_aux[['customer_id', 'avg_unique_basket_size']], how='left', on='customer_id' )\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69667/428756391.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  customers_return.loc[:, 'revenue_returned'] = customers_return.loc[:, 'quantity'] * customers_return.loc[:, 'unit_price']\n"
     ]
    }
   ],
   "source": [
    "# money returned\n",
    "customers_return = df2[df2['quantity']< 0]\n",
    "customers_return.loc[:, 'revenue_returned'] = customers_return.loc[:, 'quantity'] * customers_return.loc[:, 'unit_price']\n",
    "revenue_returned = customers_return.groupby('customer_id')['revenue_returned'].sum().reset_index()\n",
    "revenue_returned['revenue_returned'] = revenue_returned['revenue_returned']\n",
    "\n",
    "df_ref = pd.merge(df_ref, revenue_returned, on='customer_id', how='left')\n",
    "df_ref = df_ref.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = df_ref.dropna()\n",
    "df4 = df_ref.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Estudo de Espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_no_scale = df4.drop( columns=['customer_id'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4 Tree-Based Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df4_no_scale.drop( columns=['gross_revenue'], axis=1 )\n",
    "y = df4_no_scale['gross_revenue']\n",
    "\n",
    "# model definition\n",
    "rf = RandomForestRegressor( n_estimators=100,\n",
    "                           criterion='friedman_mse',\n",
    "                           random_state=42)\n",
    "# model training\n",
    "rf.fit(X, y)\n",
    "\n",
    "# dataframe leaf\n",
    "df_leaf = pd.DataFrame( rf.apply( X ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP( n_neighbors=20, random_state=42,  )\n",
    "embedding = reducer.fit_transform( df_leaf )\n",
    "\n",
    "# embedding\n",
    "df_tree = pd.DataFrame()\n",
    "df_tree['embedding_x'] = embedding[:, 0]\n",
    "df_tree['embedding_y'] = embedding[:, 1]\n",
    "# df_tree['embedding_z'] = embedding[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Hyperparameter Fine-Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df_tree.copy()\n",
    "X = df7.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Machine Learnign Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "\n",
    "# model trainig \n",
    "kmeans.fit ( X )\n",
    "\n",
    "# clustering\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSS value: 35485.94140625\n",
      "SS Value: 0.5762630105018616\n"
     ]
    }
   ],
   "source": [
    "# WSS \n",
    "print( f'WSS value: {kmeans.inertia_}')\n",
    "\n",
    "# Silhouette\n",
    "print(f'SS Value: {metrics.silhouette_score( X, labels, metric=\"euclidean\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = X.copy()\n",
    "df9['cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['cluster'] = labels\n",
    "df9 = df4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9['recency_days'] = df9['recency_days'].apply( lambda x: 1/ x if x != 0 else 0)\n",
    "df9['qtde_returns'] = df9['qtde_returns'].apply( lambda x: 1/ x if x != 0 else 0)\n",
    "df9['avg_recency_days'] = df9['avg_recency_days'].apply( lambda x: 1/ x if x != 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of customers\n",
    "df_cluster = df9[['customer_id', 'cluster']].groupby('cluster').count().reset_index()\n",
    "df_cluster['per_customer'] = 100 * df_cluster['customer_id'] / df_cluster['customer_id'].sum()\n",
    "\n",
    "# Avg revenue\n",
    "df_avg_revenue = df9[['gross_revenue', 'cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_avg_revenue, on='cluster', how='inner')\n",
    "\n",
    "# Avg recency\n",
    "df_avg_recency = df9[['recency_days', 'cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_avg_recency, on='cluster', how='inner')\n",
    "\n",
    "# purchases\n",
    "df_avg_frequency = df9[['qtde_invoices', 'cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_avg_frequency, on='cluster', how='inner')\n",
    "\n",
    "# # Avg Ticket\n",
    "# df_avg_ticket = df9[['revenue', 'cluster']].groupby('cluster').mean().reset_index()\n",
    "# df_cluster = pd.merge(df_cluster, df_avg_ticket, on='cluster', how='inner')\n",
    "\n",
    "# returns\n",
    "df_avg_ticket = df9[['qtde_returns', 'cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_avg_ticket, on='cluster', how='inner')\n",
    "\n",
    "# product_quantity\n",
    "df_avg_ticket = df9[['qtde_products', 'cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_avg_ticket, on='cluster', how='inner')\n",
    "\n",
    "# avg_frquency\n",
    "df_avg_ticket = df9[['frequency', 'cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_avg_ticket, on='cluster', how='inner')\n",
    "\n",
    "# quantity\n",
    "df_avg_ticket = df9[['avg_basket_size', 'cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_avg_ticket, on='cluster', how='inner')\n",
    "\n",
    "# AVG revenue returned\n",
    "df_avg_money_returned = df9.groupby('cluster')['revenue_returned'].mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_avg_money_returned, on='cluster', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>per_customer</th>\n",
       "      <th>gross_revenue</th>\n",
       "      <th>recency_days</th>\n",
       "      <th>qtde_invoices</th>\n",
       "      <th>qtde_returns</th>\n",
       "      <th>qtde_products</th>\n",
       "      <th>frequency</th>\n",
       "      <th>avg_basket_size</th>\n",
       "      <th>revenue_returned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>433</td>\n",
       "      <td>7.603161</td>\n",
       "      <td>12091.152102</td>\n",
       "      <td>30.050808</td>\n",
       "      <td>15.600462</td>\n",
       "      <td>336.785219</td>\n",
       "      <td>370.685912</td>\n",
       "      <td>0.138669</td>\n",
       "      <td>886.855937</td>\n",
       "      <td>-521.349607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>216</td>\n",
       "      <td>3.792801</td>\n",
       "      <td>3686.197454</td>\n",
       "      <td>119.486111</td>\n",
       "      <td>3.305556</td>\n",
       "      <td>10.180556</td>\n",
       "      <td>313.361111</td>\n",
       "      <td>0.713998</td>\n",
       "      <td>900.065978</td>\n",
       "      <td>-25.495880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>378</td>\n",
       "      <td>6.637401</td>\n",
       "      <td>2559.131958</td>\n",
       "      <td>40.230159</td>\n",
       "      <td>6.642857</td>\n",
       "      <td>21.944444</td>\n",
       "      <td>158.732804</td>\n",
       "      <td>0.099242</td>\n",
       "      <td>329.378138</td>\n",
       "      <td>-45.562778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>809</td>\n",
       "      <td>14.205443</td>\n",
       "      <td>1602.229370</td>\n",
       "      <td>64.672435</td>\n",
       "      <td>4.074166</td>\n",
       "      <td>13.105068</td>\n",
       "      <td>109.128554</td>\n",
       "      <td>0.241996</td>\n",
       "      <td>319.153840</td>\n",
       "      <td>-29.735859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>374</td>\n",
       "      <td>6.567164</td>\n",
       "      <td>1176.347727</td>\n",
       "      <td>117.334225</td>\n",
       "      <td>2.467914</td>\n",
       "      <td>5.026738</td>\n",
       "      <td>114.799465</td>\n",
       "      <td>0.561089</td>\n",
       "      <td>289.690618</td>\n",
       "      <td>-13.897299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>564</td>\n",
       "      <td>9.903424</td>\n",
       "      <td>932.056915</td>\n",
       "      <td>99.930851</td>\n",
       "      <td>2.434397</td>\n",
       "      <td>7.420213</td>\n",
       "      <td>66.781915</td>\n",
       "      <td>0.386474</td>\n",
       "      <td>244.434752</td>\n",
       "      <td>-15.991064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>376</td>\n",
       "      <td>6.602283</td>\n",
       "      <td>548.072021</td>\n",
       "      <td>97.321809</td>\n",
       "      <td>2.210106</td>\n",
       "      <td>3.678191</td>\n",
       "      <td>42.821809</td>\n",
       "      <td>0.426589</td>\n",
       "      <td>186.620966</td>\n",
       "      <td>-10.138697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>794</td>\n",
       "      <td>13.942054</td>\n",
       "      <td>442.860227</td>\n",
       "      <td>136.267003</td>\n",
       "      <td>1.714106</td>\n",
       "      <td>2.068010</td>\n",
       "      <td>41.696474</td>\n",
       "      <td>0.650830</td>\n",
       "      <td>128.424160</td>\n",
       "      <td>-7.282380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>822</td>\n",
       "      <td>14.433714</td>\n",
       "      <td>259.274185</td>\n",
       "      <td>162.371046</td>\n",
       "      <td>1.238443</td>\n",
       "      <td>1.709246</td>\n",
       "      <td>16.588808</td>\n",
       "      <td>0.828490</td>\n",
       "      <td>108.228994</td>\n",
       "      <td>-5.005401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>443</td>\n",
       "      <td>7.778753</td>\n",
       "      <td>143.865305</td>\n",
       "      <td>185.076749</td>\n",
       "      <td>1.130926</td>\n",
       "      <td>0.801354</td>\n",
       "      <td>13.550790</td>\n",
       "      <td>0.953968</td>\n",
       "      <td>36.414167</td>\n",
       "      <td>-2.852867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>486</td>\n",
       "      <td>8.533802</td>\n",
       "      <td>18.970041</td>\n",
       "      <td>197.425926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615226</td>\n",
       "      <td>3.055556</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>5.100823</td>\n",
       "      <td>-1.776975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster  customer_id  per_customer  gross_revenue  recency_days  \\\n",
       "6         6          433      7.603161   12091.152102     30.050808   \n",
       "9         9          216      3.792801    3686.197454    119.486111   \n",
       "0         0          378      6.637401    2559.131958     40.230159   \n",
       "3         3          809     14.205443    1602.229370     64.672435   \n",
       "4         4          374      6.567164    1176.347727    117.334225   \n",
       "7         7          564      9.903424     932.056915     99.930851   \n",
       "1         1          376      6.602283     548.072021     97.321809   \n",
       "8         8          794     13.942054     442.860227    136.267003   \n",
       "10       10          822     14.433714     259.274185    162.371046   \n",
       "2         2          443      7.778753     143.865305    185.076749   \n",
       "5         5          486      8.533802      18.970041    197.425926   \n",
       "\n",
       "    qtde_invoices  qtde_returns  qtde_products  frequency  avg_basket_size  \\\n",
       "6       15.600462    336.785219     370.685912   0.138669       886.855937   \n",
       "9        3.305556     10.180556     313.361111   0.713998       900.065978   \n",
       "0        6.642857     21.944444     158.732804   0.099242       329.378138   \n",
       "3        4.074166     13.105068     109.128554   0.241996       319.153840   \n",
       "4        2.467914      5.026738     114.799465   0.561089       289.690618   \n",
       "7        2.434397      7.420213      66.781915   0.386474       244.434752   \n",
       "1        2.210106      3.678191      42.821809   0.426589       186.620966   \n",
       "8        1.714106      2.068010      41.696474   0.650830       128.424160   \n",
       "10       1.238443      1.709246      16.588808   0.828490       108.228994   \n",
       "2        1.130926      0.801354      13.550790   0.953968        36.414167   \n",
       "5        1.000000      0.615226       3.055556   1.002058         5.100823   \n",
       "\n",
       "    revenue_returned  \n",
       "6        -521.349607  \n",
       "9         -25.495880  \n",
       "0         -45.562778  \n",
       "3         -29.735859  \n",
       "4         -13.897299  \n",
       "7         -15.991064  \n",
       "1         -10.138697  \n",
       "8          -7.282380  \n",
       "10         -5.005401  \n",
       "2          -2.852867  \n",
       "5          -1.776975  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cluster.sort_values('gross_revenue', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0 Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5695, 14)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Database connection details\n",
    "host = 'cluter-db.cx0iy0e6kyf8.sa-east-1.rds.amazonaws.com'\n",
    "port = '5432'\n",
    "database = 'postgres'\n",
    "user = 'natan'\n",
    "pwd = 'natan2019'\n",
    "\n",
    "# Connection string\n",
    "endpoint = f'postgresql://{user}:{pwd}@{host}:{port}/{database}'\n",
    "conn = create_engine( endpoint )\n",
    "\n",
    "# SQL query to create the table\n",
    "query_create_table_insiders = \"\"\"\n",
    "    CREATE TABLE insiders (\n",
    "        customer_id             INTEGER,\n",
    "        gross_revenue           REAL,\n",
    "        recency_days            REAL,\n",
    "        qtde_invoices           INTEGER,\n",
    "        qtde_items              INTEGER,\n",
    "        qtde_products           INTEGER,\n",
    "        avg_ticket              REAL,\n",
    "        avg_recency_days        REAL,\n",
    "        frequency               REAL,\n",
    "        qtde_returns            REAL,\n",
    "        avg_basket_size         REAL,\n",
    "        avg_unique_basket_size  REAL,\n",
    "        revenue_returned        REAL,\n",
    "        cluster                 INTEGER\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query as raw SQL\n",
    "with conn.connect() as conn:\n",
    "    conn.execute(text(query_create_table_insiders))\n",
    "\n",
    "# Assuming df9 is your DataFrame\n",
    "df9.to_sql('insiders', con=conn, if_exists='append', index=False)\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
